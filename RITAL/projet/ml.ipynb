{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NeilB\\AppData\\Local\\Temp\\ipykernel_51396\\913017747.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import utils\n",
    "import ml_functions\n",
    "\n",
    "red_code = '\\033[91m'\n",
    "blue_code = '\\033[94m'\n",
    "green_code = '\\033[92m'\n",
    "yellow_code = '\\033[93m'\n",
    "reset_code = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# word2vec_model_path = \"models/modele_word2vec.bin\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# word2vec_model = Word2Vec.load_word2vec_format(word2vec_model_path, binary=True)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m glove_vectors_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/vecteurs_glove.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m glove_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglove_vectors_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\models\\keyedvectors.py:2059\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2057\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2058\u001b[0m     header \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mto_unicode(fin\u001b[38;5;241m.\u001b[39mreadline(), encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[1;32m-> 2059\u001b[0m     vocab_size, vector_size \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m header\u001b[38;5;241m.\u001b[39msplit()]  \u001b[38;5;66;03m# throws for invalid file format\u001b[39;00m\n\u001b[0;32m   2060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m limit:\n\u001b[0;32m   2061\u001b[0m     vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(vocab_size, limit)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# word2vec_model_path = \"models/modele_word2vec.bin\"\n",
    "# word2vec_model = Word2Vec.load_word2vec_format(word2vec_model_path, binary=True)\n",
    "\n",
    "glove_vectors_path = \"models/vecteurs_glove.txt\"\n",
    "glove_vectors = KeyedVectors.load_word2vec_format(glove_vectors_path, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data & Pr√©processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./datasets/movies/movies1000/\"\n",
    "alltxts,alllabs = utils.load_movies(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.DataFrame()\n",
    "movies_df['text'] = alltxts\n",
    "movies_df['label'] = alllabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_alltxts = [utils.preprocess(alltxt) for alltxt in movies_df.text]\n",
    "\n",
    "preprocessed_movies_df = pd.DataFrame()\n",
    "preprocessed_movies_df['text'] = preprocessed_alltxts\n",
    "preprocessed_movies_df['label'] = alllabs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests & Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(analyze_function, **vectorizer_args):\n",
    "\n",
    "    print(f'{yellow_code}count{reset_code}')\n",
    "    ml_functions.count_analyze(movies_df, analyze_function, **vectorizer_args)\n",
    "\n",
    "    print(f'\\n{yellow_code}tfidf{reset_code}')\n",
    "    ml_functions.tfidf_analyze(movies_df, analyze_function, **vectorizer_args)\n",
    "\n",
    "    print(f'\\n{yellow_code}hasing{reset_code}')\n",
    "    ml_functions.hashing_analyze(movies_df, analyze_function, **vectorizer_args)\n",
    "\n",
    "    # print(f'\\n{yellow_code}word2vec{reset_code}')\n",
    "    # ml_functions.word2vec_analyze(movies_df, analyze_function, word2vec_model_path)\n",
    "\n",
    "    # print(f'\\n{yellow_code}glove{reset_code}')\n",
    "    # ml_functions.glove_analyze(movies_df, analyze_function, glove_vectors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_evaluations(analyze_function):\n",
    "\n",
    "    print(f'{blue_code}Non binary{reset_code}')\n",
    "    evaluation(analyze_function)\n",
    "\n",
    "    print(f'{blue_code}Binary{reset_code}')\n",
    "    evaluation(analyze_function, binary=True)\n",
    "\n",
    "    print(f'{blue_code}Stop word{reset_code}')\n",
    "    evaluation(analyze_function, stop_words='english')\n",
    "\n",
    "    print(f'{blue_code}R√©duction du dictionnaire{reset_code}')\n",
    "    evaluation(analyze_function, max_df=.75)\n",
    "\n",
    "    print(f'{blue_code}Bigram{reset_code}')\n",
    "    evaluation(analyze_function,ngram_range=(1, 2))\n",
    "\n",
    "    print(f'{blue_code}Trigram{reset_code}')\n",
    "    evaluation(analyze_function, ngram_range=(1, 3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R√©gresionn lin√©aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mcount\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.8525\u001b[0m\n",
      "\u001b[92mF1 score :\t0.8513853904282116\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.9217230430760769\u001b[0m\n",
      "\n",
      "\u001b[93mtfidf\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.835\u001b[0m\n",
      "\u001b[92mF1 score :\t0.8413461538461539\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.915972899322483\u001b[0m\n",
      "\n",
      "\u001b[93mhasing\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.7775\u001b[0m\n",
      "\u001b[92mF1 score :\t0.789598108747045\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8528963224080602\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "analyze_function = ml_functions.logistic_regression_analyze\n",
    "\n",
    "all_evaluations(analyze_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM lin√©aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mcount\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.7475\u001b[0m\n",
      "\u001b[92mF1 score :\t0.7292225201072386\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8470961774044351\u001b[0m\n",
      "\n",
      "\u001b[93mtfidf\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.8525\u001b[0m\n",
      "\u001b[92mF1 score :\t0.855036855036855\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.9228980724518114\u001b[0m\n",
      "\n",
      "\u001b[93mhasing\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.81\u001b[0m\n",
      "\u001b[92mF1 score :\t0.8173076923076923\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8954723868096702\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "analyze_function = ml_functions.svm_analyze\n",
    "\n",
    "evaluation(analyze_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mcount\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.665\u001b[0m\n",
      "\u001b[92mF1 score :\t0.6666666666666666\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.6649916247906197\u001b[0m\n",
      "\n",
      "\u001b[93mtfidf\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.63\u001b[0m\n",
      "\u001b[92mF1 score :\t0.6390243902439025\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.6298907472686818\u001b[0m\n",
      "\n",
      "\u001b[93mhasing\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.6525\u001b[0m\n",
      "\u001b[92mF1 score :\t0.6445012787723785\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.652628815720393\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "analyze_function = ml_functions.decision_tree_analyze\n",
    "\n",
    "evaluation(analyze_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mcount\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.7875\u001b[0m\n",
      "\u001b[92mF1 score :\t0.7858942065491183\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8660341508537714\u001b[0m\n",
      "\n",
      "\u001b[93mtfidf\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.8\u001b[0m\n",
      "\u001b[92mF1 score :\t0.7905759162303665\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8813220330508262\u001b[0m\n",
      "\n",
      "\u001b[93mhasing\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.77\u001b[0m\n",
      "\u001b[92mF1 score :\t0.7688442211055276\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8677716942923572\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "analyze_function = ml_functions.random_forest_analyze\n",
    "\n",
    "evaluation(analyze_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R√©gresionn lin√©aire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Pr√©processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mwithout preprocessing vectorizer Count\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'ml_functions' has no attribute 'count_analyze_logistic_regression'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myellow_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mwithout preprocessing vectorizer Count\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreset_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mml_functions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_analyze_logistic_regression\u001b[49m(movies_df)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'ml_functions' has no attribute 'count_analyze_logistic_regression'"
     ]
    }
   ],
   "source": [
    "print(f'{yellow_code}without preprocessing vectorizer Count{reset_code}')\n",
    "ml_functions.count_analyze_logistic_regression(movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mwithout preprocessing vectorizer TfIdf\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.835\u001b[0m\n",
      "\u001b[92mF1 score :\t0.8413461538461539\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.915972899322483\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f'{yellow_code}without preprocessing vectorizer TfIdf{reset_code}')\n",
    "ml_functions.tfidf_analyze_logistic_regression(movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mwithout preprocessing vectorizer Count Bi-gram\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.8625\u001b[0m\n",
      "\u001b[92mF1 score :\t0.8635235732009926\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.9302482562064052\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f'{yellow_code}without preprocessing vectorizer Count Bi-gram{reset_code}')\n",
    "ml_functions.count_analyze_logistic_regression(movies_df,ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mwithout preprocessing vectorizer Count Tri-gram\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.75\u001b[0m\n",
      "\u001b[92mF1 score :\t0.7237569060773481\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8317207930198256\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f'{yellow_code}without preprocessing vectorizer Count Tri-gram{reset_code}')\n",
    "ml_functions.count_analyze_logistic_regression(movies_df,ngram_range=(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With pr√©processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mwith preprocessing vectorizer TfIdf\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.835\u001b[0m\n",
      "\u001b[92mF1 score :\t0.835\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.9159978999474987\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f'{yellow_code}with preprocessing vectorizer TfIdf{reset_code}')\n",
    "ml_functions.count_analyze_logistic_regression(preprocessed_movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mwith preprocessing vectorizer TfIdf\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.8325\u001b[0m\n",
      "\u001b[92mF1 score :\t0.8385542168674699\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.9093977349433736\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f'{yellow_code}with preprocessing vectorizer TfIdf{reset_code}')\n",
    "ml_functions.tfidf_analyze_logistic_regression(preprocessed_movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mwith preprocessing vectorizer Count Bi-gram\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.81\u001b[0m\n",
      "\u001b[92mF1 score :\t0.8118811881188119\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8959973999349984\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f'{yellow_code}with preprocessing vectorizer Count Bi-gram{reset_code}')\n",
    "ml_functions.count_analyze_logistic_regression(preprocessed_movies_df,ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mwith preprocessing vectorizer Count Tri-gram\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.7625\u001b[0m\n",
      "\u001b[92mF1 score :\t0.7425474254742548\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8332208305207631\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f'{yellow_code}with preprocessing vectorizer Count Tri-gram{reset_code}')\n",
    "ml_functions.count_analyze_logistic_regression(preprocessed_movies_df,ngram_range=(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import (\n",
    "    linear_model, \n",
    "    ensemble,\n",
    "    tree,\n",
    "    decomposition, \n",
    "    naive_bayes, \n",
    "    neural_network,\n",
    "    svm,\n",
    "    metrics,\n",
    "    preprocessing, \n",
    "    model_selection, \n",
    "    pipeline,\n",
    ")\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, auc, roc_curve\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les donn√©es en ensembles d'entra√Ænement et de test\n",
    "X_text_train, X_text_test, y_train, y_test = model_selection.train_test_split(movies_df['text'], movies_df['label'], test_size=0.2)\n",
    "#vectorizer = CountVectorizer(ngram_range=(2,2))   #Mieux  en gardant les stopwords bizzare |bigram marche VRAIMENT bien, trigram impossible a faire pas assez de memoire :c ) \n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "X_text_train = vectorizer.fit_transform(X_text_train)\n",
    "X_text_test = vectorizer.transform(X_text_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "#scaler = StandardScaler() moins efficace que MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_text_train.toarray())\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_text_train = scaler.transform(X_text_train.toarray())\n",
    "X_text_test = scaler.transform(X_text_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_c = 100\n",
    "pca = PCA(.99)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_text_train)\n",
    "X_test_pca = pca.transform(X_text_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAccuracy :\t0.89\u001b[0m\n",
      "\u001b[92mF1 score :\t0.8916256157635468\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.9558238955973899\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = clf = linear_model.LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "# Entra√Æner le mod√®le sur les donn√©es d'entra√Ænement\n",
    "model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Pr√©dire les √©tiquettes des donn√©es de test\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Pr√©dire les probabilit√©s des classes positives pour les donn√©es de test\n",
    "# Pr√©dire les probabilit√©s des classes positives pour les donn√©es de test\n",
    "if hasattr(model, \"predict_proba\"):\n",
    "    y_prob = model.predict_proba(X_test_pca)[:, 1]\n",
    "else:\n",
    "    # Utiliser la d√©cision de fonction de d√©cision si le mod√®le ne prend pas en charge predict_proba\n",
    "    y_prob = model.decision_function(X_test_pca)\n",
    "\n",
    "# Calcul des m√©triques de performance\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Affichage du rapport de classification\n",
    "report = metrics.classification_report(y_test, y_pred)\n",
    "# print(report)\n",
    "\n",
    "print(f'{green_code}Accuracy :\\t{acc}{reset_code}')\n",
    "print(f'{green_code}F1 score :\\t{f1}{reset_code}')\n",
    "print(f'{green_code}AUC :\\t\\t{auc}{reset_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xshape  =  (1600, 457235)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "# Diviser les donn√©es en ensembles d'entra√Ænement et de test\n",
    "X_text_train, X_text_test, y_train, y_test = model_selection.train_test_split(movies_df['text'], movies_df['label'], test_size=0.2, random_state=42)\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))   #Mieux  en gardant les stopwords bizzare\n",
    "#vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X_text_train = vectorizer.fit_transform(X_text_train)\n",
    "X_text_test = vectorizer.transform(X_text_test)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_text_train = scaler.fit_transform(X_text_train.toarray())\n",
    "X_text_test = scaler.transform(X_text_test.toarray())\n",
    "print(\"xshape  = \", X_text_train.shape)\n",
    "# Create a TruncatedSVD object and fit the data\n",
    "svd = TruncatedSVD(n_components=1600)\n",
    "X_svd_train = svd.fit_transform(X_text_train)\n",
    "X_svd_test = svd.transform(X_text_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAccuracy :\t0.9075\u001b[0m\n",
      "\u001b[92mF1 score :\t0.9053708439897699\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.9551238780969525\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = clf = linear_model.LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "# Entra√Æner le mod√®le sur les donn√©es d'entra√Ænement\n",
    "model.fit(X_svd_train, y_train)\n",
    "\n",
    "# Pr√©dire les √©tiquettes des donn√©es de test\n",
    "y_pred = model.predict(X_svd_test)\n",
    "\n",
    "# Pr√©dire les probabilit√©s des classes positives pour les donn√©es de test\n",
    "# Pr√©dire les probabilit√©s des classes positives pour les donn√©es de test\n",
    "if hasattr(model, \"predict_proba\"):\n",
    "    y_prob = model.predict_proba(X_svd_test)[:, 1]\n",
    "else:\n",
    "    # Utiliser la d√©cision de fonction de d√©cision si le mod√®le ne prend pas en charge predict_proba\n",
    "    y_prob = model.decision_function(X_svd_test)\n",
    "\n",
    "# Calcul des m√©triques de performance\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Affichage du rapport de classification\n",
    "report = metrics.classification_report(y_test, y_pred)\n",
    "# print(report)\n",
    "\n",
    "print(f'{green_code}Accuracy :\\t{acc}{reset_code}')\n",
    "print(f'{green_code}F1 score :\\t{f1}{reset_code}')\n",
    "print(f'{green_code}AUC :\\t\\t{auc}{reset_code}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
