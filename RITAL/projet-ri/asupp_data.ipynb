{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "from datasets import load_from_disk\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "from utils import preprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des des données et Prétraitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiqa_data = load_from_disk(\"wikiqa\")\n",
    "test_data_set = wikiqa_data[\"test\"]\n",
    "train_data_set = wikiqa_data[\"train\"]\n",
    "validation_data_set = wikiqa_data[\"validation\"]\n",
    "\n",
    "\n",
    "def preprocess_examples(examples):\n",
    "    examples['question'] = [preprocess(q) for q in examples['question']]\n",
    "    examples['answer'] = [preprocess(a) for a in examples['answer']]\n",
    "    return examples\n",
    "\n",
    "train_data_set = train_data_set.map(preprocess_examples, batched=True)\n",
    "validation_data_set = validation_data_set.map(preprocess_examples, batched=True)\n",
    "test_data_set = test_data_set.map(preprocess_examples, batched=True)\n",
    "\n",
    "# Convertir en DataFrame\n",
    "train_df = pd.DataFrame({\n",
    "    'question': train_data_set['question'],\n",
    "    'answer': train_data_set['answer'],\n",
    "    'label': train_data_set['label']\n",
    "})\n",
    "\n",
    "validation_df = pd.DataFrame({\n",
    "    'question': validation_data_set['question'],\n",
    "    'answer': validation_data_set['answer'],\n",
    "    'label': validation_data_set['label']\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'question': test_data_set['question'],\n",
    "    'answer': test_data_set['answer'],\n",
    "    'label': test_data_set['label']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble de données d'entraînement :\n",
      "label\n",
      "0    19320\n",
      "1     1040\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Ensemble de données de validation :\n",
      "label\n",
      "0    2593\n",
      "1     140\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Ensemble de données de test :\n",
      "label\n",
      "0    5872\n",
      "1     293\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Afficher les statistiques des ensembles de données après suréchantillonnage\n",
    "print(\"Ensemble de données d'entraînement :\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(\"\\nEnsemble de données de validation :\")\n",
    "print(validation_df['label'].value_counts())\n",
    "print(\"\\nEnsemble de données de test :\")\n",
    "print(test_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les questions sans réponse pertinente\n",
    "def filter_non_relevant(df):\n",
    "    relevant_questions = df[df['label'] == 1]['question'].unique()\n",
    "    return df[df['question'].isin(relevant_questions)]\n",
    "\n",
    "train_df = filter_non_relevant(train_df)\n",
    "validation_df = filter_non_relevant(validation_df)\n",
    "test_df = filter_non_relevant(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble de données d'entraînement :\n",
      "label\n",
      "0    7645\n",
      "1    1040\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Ensemble de données de validation :\n",
      "label\n",
      "0    990\n",
      "1    140\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Ensemble de données de test :\n",
      "label\n",
      "0    2058\n",
      "1     293\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Afficher les statistiques des ensembles de données après suréchantillonnage\n",
    "print(\"Ensemble de données d'entraînement :\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(\"\\nEnsemble de données de validation :\")\n",
    "print(validation_df['label'].value_counts())\n",
    "print(\"\\nEnsemble de données de test :\")\n",
    "print(test_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regrouper les réponses et les labels pour chaque question\n",
    "def group_answers(df):\n",
    "    grouped = df.groupby('question').agg(list).reset_index()\n",
    "    return grouped\n",
    "\n",
    "train_grouped = group_answers(train_df)\n",
    "validation_grouped = group_answers(validation_df)\n",
    "test_grouped = group_answers(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how a rocket engine works</td>\n",
       "      <td>[rs being tested at nasas stennis space center...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how are aircraft radial engines built</td>\n",
       "      <td>[radial engine timing and cam mechanism, click...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how are cholera and typhus transmitted and pre...</td>\n",
       "      <td>[cholera is an infection in the small intestin...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how are glacier caves formed</td>\n",
       "      <td>[a partly submerged glacier cave on perito mor...</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how are public schools funded</td>\n",
       "      <td>[state schools also known as public schools or...</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>who wrote the song cocaine</td>\n",
       "      <td>[cocaine is a song written and recorded by jj ...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>who wrote the song feelin alright</td>\n",
       "      <td>[feelin alright also known as feeling alright ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>who wrote the song in the mood</td>\n",
       "      <td>[in the mood is a big band era hit recorded by...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>who wrote whats my name rihanna</td>\n",
       "      <td>[whats my name is a song recorded by barbadian...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>who wrote white christmas</td>\n",
       "      <td>[thumb, white christmas is an irving berlin so...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>871 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0                            how a rocket engine works   \n",
       "1                how are aircraft radial engines built   \n",
       "2    how are cholera and typhus transmitted and pre...   \n",
       "3                         how are glacier caves formed   \n",
       "4                        how are public schools funded   \n",
       "..                                                 ...   \n",
       "866                         who wrote the song cocaine   \n",
       "867                  who wrote the song feelin alright   \n",
       "868                     who wrote the song in the mood   \n",
       "869                    who wrote whats my name rihanna   \n",
       "870                          who wrote white christmas   \n",
       "\n",
       "                                                answer  \\\n",
       "0    [rs being tested at nasas stennis space center...   \n",
       "1    [radial engine timing and cam mechanism, click...   \n",
       "2    [cholera is an infection in the small intestin...   \n",
       "3    [a partly submerged glacier cave on perito mor...   \n",
       "4    [state schools also known as public schools or...   \n",
       "..                                                 ...   \n",
       "866  [cocaine is a song written and recorded by jj ...   \n",
       "867  [feelin alright also known as feeling alright ...   \n",
       "868  [in the mood is a big band era hit recorded by...   \n",
       "869  [whats my name is a song recorded by barbadian...   \n",
       "870  [thumb, white christmas is an irving berlin so...   \n",
       "\n",
       "                                 label  \n",
       "0             [0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "1                [0, 0, 0, 0, 1, 0, 0]  \n",
       "2          [0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "3                      [0, 0, 0, 1, 0]  \n",
       "4                               [1, 1]  \n",
       "..                                 ...  \n",
       "866                    [1, 0, 0, 0, 0]  \n",
       "867                 [1, 0, 0, 0, 0, 0]  \n",
       "868                          [1, 0, 0]  \n",
       "869  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "870              [0, 1, 0, 0, 0, 0, 0]  \n",
       "\n",
       "[871 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion des questions et réponses en Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "def embed_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "    word_vecs = [model[word] for word in words if word in model]\n",
    "    return np.mean(word_vecs, axis=0) if word_vecs else np.zeros(model.vector_size)\n",
    "\n",
    "train_grouped['question_vec'] = train_grouped['question'].apply(embed_sentence)\n",
    "train_grouped['answer_vecs'] = train_grouped['answer'].apply(lambda answers: [embed_sentence(a) for a in answers])\n",
    "\n",
    "validation_grouped['question_vec'] = validation_grouped['question'].apply(embed_sentence)\n",
    "validation_grouped['answer_vecs'] = validation_grouped['answer'].apply(lambda answers: [embed_sentence(a) for a in answers])\n",
    "\n",
    "test_grouped['question_vec'] = test_grouped['question'].apply(embed_sentence)\n",
    "test_grouped['answer_vecs'] = test_grouped['answer'].apply(lambda answers: [embed_sentence(a) for a in answers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QARankingDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.questions = np.stack(df['question_vec'].values)\n",
    "        self.answers = np.stack(df['answer_vec'].values)\n",
    "        self.labels = df['label'].values\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        self.features = np.hstack([\n",
    "            self.questions,\n",
    "            self.answers\n",
    "        ])\n",
    "\n",
    "        self.features = self.scaler.fit_transform(self.features)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question_feature = torch.tensor(self.features[idx][:len(self.features[idx])//2], dtype=torch.float32)\n",
    "        answer_feature = torch.tensor(self.features[idx][len(self.features[idx])//2:], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return question_feature, answer_feature, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du modèle SVM pour le ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankSVM(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RankSVM, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim * 2, 256)  # input_dim * 2 car on concatène question et réponse\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, question, answer):\n",
    "        combined = torch.cat((question, answer), dim=1)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        score = self.fc3(x)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement et évaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ranking_model(model, train_loader, validation_loader, criterion, optimizer, num_epochs=10, patience=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    trigger_times = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for question, answer, labels in train_loader:\n",
    "            question, answer, labels = question.to(device), answer.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scores = model(question, answer)\n",
    "            loss = criterion(scores, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for question, answer, labels in validation_loader:\n",
    "                question, answer, labels = question.to(device), answer.to(device), labels.to(device)\n",
    "                scores = model(question, answer)\n",
    "                loss = criterion(scores, labels.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(validation_loader)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            torch.save(model.state_dict(), 'best_rank_model.pth')\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            print(f\"Trigger Times: {trigger_times}\")\n",
    "            \n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!')\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Loss: 0.1131\n",
      "Validation Loss: 0.1038\n",
      "Epoch [2], Loss: 0.1014\n",
      "Validation Loss: 0.1065\n",
      "Trigger Times: 1\n",
      "Epoch [3], Loss: 0.0972\n",
      "Validation Loss: 0.1012\n",
      "Epoch [4], Loss: 0.0941\n",
      "Validation Loss: 0.1016\n",
      "Trigger Times: 1\n",
      "Epoch [5], Loss: 0.0896\n",
      "Validation Loss: 0.1037\n",
      "Trigger Times: 2\n",
      "Epoch [6], Loss: 0.0885\n",
      "Validation Loss: 0.1005\n",
      "Epoch [7], Loss: 0.0858\n",
      "Validation Loss: 0.0998\n",
      "Epoch [8], Loss: 0.0840\n",
      "Validation Loss: 0.1045\n",
      "Trigger Times: 1\n",
      "Epoch [9], Loss: 0.0795\n",
      "Validation Loss: 0.1030\n",
      "Trigger Times: 2\n",
      "Epoch [10], Loss: 0.0769\n",
      "Validation Loss: 0.1026\n",
      "Trigger Times: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Préparation des données\n",
    "train_dataset = QARankingDataset(train_df)\n",
    "validation_dataset = QARankingDataset(validation_df)\n",
    "test_dataset = QARankingDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialisation du modèle\n",
    "input_dim = train_dataset.features.shape[1] // 2  # Divisé par 2 car nous concaténons les questions et les réponses\n",
    "model = RankSVM(input_dim)\n",
    "\n",
    "# Définition de la perte et de l'optimiseur\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entraînement du modèle\n",
    "train_ranking_model(model, train_loader, validation_loader, criterion, optimizer, num_epochs=10, patience=5)\n",
    "\n",
    "# Charger le meilleur modèle sauvegardé\n",
    "model.load_state_dict(torch.load('best_rank_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "MAP: 0.5\n",
      "MRR: 0.0019494389662252029\n",
      "Success@1: 0.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kadem\\.pyenv\\pyenv-win\\versions\\3.9.9\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def compute_ranking_metrics(labels, scores):\n",
    "    from sklearn.metrics import average_precision_score, label_ranking_average_precision_score\n",
    "    from scipy.stats import rankdata\n",
    "    \n",
    "    # Assuming each label is associated with a unique question ID\n",
    "    unique_questions = np.unique(labels)\n",
    "    map_scores = []\n",
    "    mrr_scores = []\n",
    "    success_at_1_scores = []\n",
    "    \n",
    "    for question in unique_questions:\n",
    "        question_indices = np.where(labels == question)[0]\n",
    "        question_scores = scores[question_indices]\n",
    "        question_labels = labels[question_indices]\n",
    "        \n",
    "        ap = average_precision_score(question_labels, question_scores)\n",
    "        map_scores.append(ap)\n",
    "        \n",
    "        # Calculate MRR\n",
    "        sorted_indices = np.argsort(-question_scores)\n",
    "        sorted_labels = question_labels[sorted_indices]\n",
    "        ranks = rankdata(-sorted_labels, method='max')\n",
    "        mrr = 1.0 / ranks[0]\n",
    "        mrr_scores.append(mrr)\n",
    "        \n",
    "        # Calculate Success@1\n",
    "        success_at_1 = 1 if sorted_labels[0] == 1 else 0\n",
    "        success_at_1_scores.append(success_at_1)\n",
    "    \n",
    "    map_score = np.mean(map_scores)\n",
    "    mrr_score = np.mean(mrr_scores)\n",
    "    success_at_1_score = np.mean(success_at_1_scores)\n",
    "    \n",
    "    return map_score, mrr_score, success_at_1_score\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for question, answer, labels in test_loader:\n",
    "            question, answer, labels = question.to(device), answer.to(device), labels.to(device)\n",
    "            scores = model(question, answer)\n",
    "            all_scores.extend(scores.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_scores = np.array(all_scores).flatten()\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Calculate ranking metrics such as MAP, MRR, and Success@1\n",
    "    map_score, mrr_score, success_at_1_score = compute_ranking_metrics(all_labels, all_scores)\n",
    "    print(f\"\\nTest Results:\\nMAP: {map_score}\\nMRR: {mrr_score}\\nSuccess@1: {success_at_1_score}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Évaluer le modèle sur les données de test\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Loss: 0.0825\n",
      "Validation Loss: 0.1006\n",
      "Epoch [2], Loss: 0.0802\n",
      "Validation Loss: 0.1012\n",
      "Trigger Times: 1\n",
      "Epoch [3], Loss: 0.0775\n",
      "Validation Loss: 0.1007\n",
      "Trigger Times: 2\n",
      "Epoch [4], Loss: 0.0738\n",
      "Validation Loss: 0.1023\n",
      "Trigger Times: 3\n",
      "Epoch [5], Loss: 0.0722\n",
      "Validation Loss: 0.1050\n",
      "Trigger Times: 4\n",
      "Epoch [6], Loss: 0.0700\n",
      "Validation Loss: 0.1063\n",
      "Trigger Times: 5\n",
      "Early stopping!\n",
      "\n",
      "Test Results:\n",
      "MAP: 0.5\n",
      "MRR: 0.0019494389662252029\n",
      "Success@1: 0.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kadem\\.pyenv\\pyenv-win\\versions\\3.9.9\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Entraînement du modèle\n",
    "train_ranking_model(model, train_loader, validation_loader, criterion, optimizer, num_epochs=10, patience=5)\n",
    "\n",
    "# Charger le meilleur modèle sauvegardé\n",
    "model.load_state_dict(torch.load('best_rank_model.pth'))\n",
    "\n",
    "# Évaluer le modèle sur les données de test\n",
    "evaluate_model(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
