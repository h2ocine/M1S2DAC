{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOr-DwYgRsUa"
      },
      "source": [
        "# Web Scrapping in Python\n",
        "\n",
        "\n",
        "**TODO pour moi avant le TME**\n",
        "\n",
        "1.   Faire la version étudiant en enlevant les réponses. Ne garder qu'un code générique pour la carte qu'ils devront adapter\n",
        "\n",
        "\n",
        "\n",
        "The first objective of this notebook is to discover the `request` and `BeautifulSoup` libraries to crawl a table on a Wikitable page, build a dataframe, and create a map.\n",
        "\n",
        "*   `request` and [urllib](https://docs.python.org/3/library/urllib.html#module-urllib) for requestion REST API\n",
        "*   [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to inspect webpages\n",
        "\n",
        "Please note that this part serves as an initiation of web scrapping but you will need to learn by yourself to make the project. It is inspired from notebooks published by Galiana, Lino. 2023. Python Pour La Data Science. https://doi.org/10.5281/zenodo.8229676. Exercices target different sources of information, the code needs to be adapted.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E4zarSeAQdHm"
      },
      "outputs": [],
      "source": [
        "!pip install -q lxml\n",
        "\n",
        "import bs4\n",
        "import lxml\n",
        "import pandas as pd\n",
        "import urllib\n",
        "import numpy as np\n",
        "\n",
        "from urllib import request"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4wzROqzT36G"
      },
      "source": [
        "## Exercice 1 : Scrap a table on a wikipedia page\n",
        "\n",
        "We would like to display on a map the location of Summer Olympic Games since 1896. We will use a [Wikipedia page](https://fr.wikipedia.org/wiki/Jeux_olympiques) to scrap the associated table.\n",
        "Below is the code to extract the content of the page using `request` and display its title using `BeautifulSoup`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl9QICzwTGG7",
        "outputId": "a6cfaba6-644e-43f8-c4bf-412c14d6914c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'<!DOCTYPE html>\\n<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-client-prefs-pinned-disabled vector-toc-available\" lang=\"fr\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\">\\n<title>Jeux olympiques \\xe2\\x80\\x94 Wikip\\xc3\\xa9dia</title>\\n<script>(function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-w'\n",
            "<title>Jeux olympiques — Wikipédia</title>\n"
          ]
        }
      ],
      "source": [
        "jo = \"https://fr.wikipedia.org/wiki/Jeux_olympiques\"\n",
        "\n",
        "request_text = request.urlopen(jo).read()\n",
        "print(request_text[:1000])\n",
        "page = bs4.BeautifulSoup(request_text, \"lxml\")\n",
        "print(page.find(\"title\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_18h3E9UiYR"
      },
      "source": [
        "Our objective here is to extract the different information in the first table \"Jeux olympiques d'été\" and to build a data frame.\n",
        "\n",
        "To proceed, you will have to follow these steps:\n",
        "*   Find the table\n",
        "*   Collect each line of the table\n",
        "*   Retrieve the different columns and transform them into text format. Also, use `strip` to format the value into a proper text format (e.g., without useless spaces). Store these lines (formated as a table of columns) in a table.\n",
        "\n",
        "**Warning: We have two location for 1940. You have therefore to manage this case: identify the attribute indicating this fact and write a code to collect those lines and insert in the previous table in the right place**\n",
        "\n",
        "* Collect headers of the HTML table\n",
        "* Build a data frame from the result table and the headers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5oVOOZuKxD_",
        "outputId": "3c21feed-ee87-42fe-8e85-451af90c0a04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "line :  ['1940', 'XII(annulés[NB 4])', 'Helsinki', 'Finlande', 'Europe']\n",
            "   Année Olympiade(Nº de l’olympiade[NB 1])         Ville hôte  \\\n",
            "0   1896                                  I         Athènes(1)   \n",
            "1   1900                                 II           Paris(1)   \n",
            "2   1904                                III     Saint-Louis(1)   \n",
            "3   1906             •(Intercalaires[NB 2])         Athènes(2)   \n",
            "4   1908                                 IV         Londres(1)   \n",
            "5   1912                                  V       Stockholm(1)   \n",
            "6   1916                  VI(annulés[NB 3])             Berlin   \n",
            "7   1920                                VII          Anvers(1)   \n",
            "8   1924                               VIII           Paris(2)   \n",
            "9   1928                                 IX       Amsterdam(1)   \n",
            "10  1932                                  X     Los Angeles(1)   \n",
            "11  1936                                 XI          Berlin(1)   \n",
            "12  1940                 XII(annulés[NB 4])              Tokyo   \n",
            "13  1940                 XII(annulés[NB 4])           Helsinki   \n",
            "14  1944                XIII(annulés[NB 4])            Londres   \n",
            "15  1948                                XIV         Londres(2)   \n",
            "16  1952                                 XV        Helsinki(1)   \n",
            "17  1956                                XVI       Melbourne(1)   \n",
            "18  1960                               XVII            Rome(1)   \n",
            "19  1964                              XVIII           Tokyo(1)   \n",
            "20  1968                                XIX          Mexico(1)   \n",
            "21  1972                                 XX          Munich(1)   \n",
            "22  1976                                XXI        Montréal(1)   \n",
            "23  1980                               XXII          Moscou(1)   \n",
            "24  1984                              XXIII     Los Angeles(2)   \n",
            "25  1988                               XXIV           Séoul(1)   \n",
            "26  1992                                XXV       Barcelone(1)   \n",
            "27  1996                               XXVI         Atlanta(1)   \n",
            "28  2000                              XXVII          Sydney(1)   \n",
            "29  2004                             XXVIII         Athènes(3)   \n",
            "30  2008                               XXIX           Pékin(1)   \n",
            "31  2012                                XXX         Londres(3)   \n",
            "32  2016                               XXXI  Rio de Janeiro(1)   \n",
            "33  2021                        XXXII[NB 5]           Tokyo(2)   \n",
            "34  2024                             XXXIII           Paris(3)   \n",
            "35  2028                              XXXIV     Los Angeles(3)   \n",
            "\n",
            "                       Pays            Continent  \n",
            "0                  Grèce(1)            Europe(1)  \n",
            "1                 France(1)            Europe(2)  \n",
            "2             États-Unis(1)  Amérique du Nord(1)  \n",
            "3                  Grèce(2)               Europe  \n",
            "4            Royaume-Uni(1)            Europe(3)  \n",
            "5                  Suède(1)            Europe(4)  \n",
            "6                 Allemagne               Europe  \n",
            "7               Belgique(1)            Europe(5)  \n",
            "8                 France(2)            Europe(6)  \n",
            "9               Pays-Bas(1)            Europe(7)  \n",
            "10            États-Unis(2)  Amérique du Nord(2)  \n",
            "11             Allemagne(1)            Europe(8)  \n",
            "12                    Japon                 Asie  \n",
            "13                 Finlande               Europe  \n",
            "14              Royaume-Uni               Europe  \n",
            "15           Royaume-Uni(2)            Europe(9)  \n",
            "16              Finlande(1)           Europe(10)  \n",
            "17             Australie(1)           Océanie(1)  \n",
            "18                Italie(1)           Europe(11)  \n",
            "19                 Japon(1)              Asie(1)  \n",
            "20               Mexique(1)  Amérique du Nord(3)  \n",
            "21  Allemagne de l'Ouest(2)           Europe(12)  \n",
            "22                Canada(1)  Amérique du Nord(4)  \n",
            "23      Union soviétique(1)           Europe(13)  \n",
            "24            États-Unis(3)  Amérique du Nord(5)  \n",
            "25          Corée du Sud(1)              Asie(2)  \n",
            "26               Espagne(1)           Europe(14)  \n",
            "27            États-Unis(4)  Amérique du Nord(6)  \n",
            "28             Australie(2)           Océanie(2)  \n",
            "29                 Grèce(3)           Europe(15)  \n",
            "30                 Chine(1)              Asie(3)  \n",
            "31           Royaume-Uni(3)           Europe(16)  \n",
            "32                Brésil(1)   Amérique du Sud(1)  \n",
            "33                 Japon(2)              Asie(4)  \n",
            "34                France(3)           Europe(17)  \n",
            "35            États-Unis(5)  Amérique du Nord(7)  \n"
          ]
        }
      ],
      "source": [
        "#[student]\n",
        "\n",
        "page = bs4.BeautifulSoup(request_text, \"lxml\")\n",
        "table_found = str(page.find(\"table\", {\"class\" : \"wikitable center\" }))\n",
        "table = bs4.BeautifulSoup(table_found, \"lxml\")\n",
        "lines = table.findAll(\"tr\")\n",
        "\n",
        "collumn_names = []\n",
        "data = []\n",
        "\n",
        "names = lines[0].findAll(\"th\")\n",
        "\n",
        "collumn_names = [th.get_text(strip=True) for th in names]\n",
        "val = 0\n",
        "for i in range(1, len(lines)):\n",
        "\n",
        "  n_line = lines[i].findAll(\"td\")\n",
        "  val -= 1\n",
        "  # Vérifier si on a plusieurs villes/pays/contients\n",
        "  if(n_line[0].has_attr(\"rowspan\")):\n",
        "    tmp_info_line = [td.get_text(strip=True) for td in n_line]\n",
        "    data.append(tmp_info_line)\n",
        "\n",
        "    val = int(n_line[0]['rowspan'])\n",
        "\n",
        "    # Recupérer les autres villes/pays/contients\n",
        "    for next in range(1, val):\n",
        "\n",
        "      line = [tmp_info_line[0],tmp_info_line[1]]\n",
        "\n",
        "      for td in lines[i+next]:\n",
        "        td_string = td.get_text(strip=True)\n",
        "        if(td_string):\n",
        "          line.append(td_string)\n",
        "      print(\"line : \", line)\n",
        "      data.append(line)\n",
        "      \n",
        "  elif val <= 0:\n",
        "    data.append([td.get_text(strip=True) for td in n_line])\n",
        "\n",
        "\n",
        "data = np.asarray(data[:-1])\n",
        "df = pd.DataFrame(data[:-1], columns=collumn_names)\n",
        "print(df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw6i-yLueZvG"
      },
      "source": [
        "## Exercice 2: Locate places on a map\n",
        "\n",
        "The objective here is to identify organizer cities on a map.\n",
        "You will have to code the following steps:\n",
        "* Collect the URL of each city\n",
        "* Go to this page using `urllib.request`\n",
        "* Find the coordinates\n",
        "* Store cities and coordinates in a data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7xaYx79JMF_N"
      },
      "outputs": [],
      "source": [
        "#[student]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gqYroUff1UQ"
      },
      "source": [
        "We then transform degree coordinates into numerical coordinates to build a map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "SPkGBLnVWZWc",
        "outputId": "846410e3-bdc8-4597-fc30-78b5a96b6563"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def dms2dd(degrees, minutes, seconds, direction):\n",
        "    dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60);\n",
        "    if direction in ('S', 'O'):\n",
        "        dd *= -1\n",
        "    return dd\n",
        "\n",
        "def parse_dms(dms):\n",
        "    parts = re.split('[^\\d\\w]+', dms)\n",
        "    #potentially needs an adaptation\n",
        "    #[student]\n",
        "    lat = dms2dd(parts[0], parts[1], parts[2], parts[3])\n",
        "    #lng = dms2dd(parts[4], parts[5], parts[6], parts[7])\n",
        "    return lat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib.parse\n",
        "\n",
        "def transform_wikipedia_url(wiki_path):\n",
        "    # Decode the URL path\n",
        "    decoded_path = urllib.parse.unquote(wiki_path)\n",
        "    \n",
        "    # Check if the path starts with \"/wiki/\"\n",
        "    if decoded_path.startswith(\"/wiki/\"):\n",
        "        # Extract the article title\n",
        "        article_title = decoded_path[len(\"/wiki/\"):]\n",
        "        \n",
        "        # Encode the non-ASCII characters in the article title\n",
        "        encoded_title = urllib.parse.quote(article_title)\n",
        "        \n",
        "        # Construct the transformed URL\n",
        "        transformed_url = f\"https://fr.wikipedia.org/wiki/{encoded_title}\"\n",
        "        \n",
        "        return transformed_url\n",
        "    else:\n",
        "        # Return the original path if it doesn't start with \"/wiki/\"\n",
        "        return wiki_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_coordinates = []\n",
        "\n",
        "page = bs4.BeautifulSoup(request_text, \"lxml\")\n",
        "table_found = str(page.find(\"table\", {\"class\" : \"wikitable center\" }))\n",
        "\n",
        "table = bs4.BeautifulSoup(table_found, \"lxml\")\n",
        "lines = table.findAll(\"tr\")\n",
        "\n",
        "hrefs = [] \n",
        "for i in range(1, len(lines)):\n",
        "\n",
        "  n_line = lines[i].findAll(\"td\")\n",
        "\n",
        "  if len(n_line) >= 3:\n",
        "      # Récupérer la balise de la ville\n",
        "      td_city = n_line[2]\n",
        "      td = bs4.BeautifulSoup(str(td_city), \"lxml\")\n",
        "\n",
        "      # Récupérer les balises a de la ligne\n",
        "      a_tag_list = td.find(\"a\")\n",
        "\n",
        "      if(a_tag_list):\n",
        "        # Récupérer la ref de la page wikepedia de la ville \n",
        "        href = a_tag_list.get('href')\n",
        "\n",
        "        # Récupérer l'url de la ville\n",
        "        url = transform_wikipedia_url(href)\n",
        "        response = urllib.request.urlopen(url)\n",
        "\n",
        "        # Lire le contenu de la page web de la ville\n",
        "        html_content = response.read()\n",
        "\n",
        "\n",
        "\n",
        "#data['latitude'] = data['latitude'].apply(parse_dms)\n",
        "##data['longitude'] = data['longitude'].apply(parse_dms)\n",
        "# print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3_3JmXggIEk"
      },
      "source": [
        "The map can be obtain with the following code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "93_lzZI3Wkpt"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'geopandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfolium\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
          ]
        }
      ],
      "source": [
        "import geopandas as gpd\n",
        "from pathlib import Path\n",
        "import folium\n",
        "from IPython.display import display\n",
        "\n",
        "gdf = gpd.GeoDataFrame(\n",
        "    data, geometry=gpd.points_from_xy(data.longitude, data.latitude))\n",
        "\n",
        "Path(\"leaflet\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "center = gdf[['latitude', 'longitude']].mean().values.tolist()\n",
        "sw = gdf[['latitude', 'longitude']].min().values.tolist()\n",
        "ne = gdf[['latitude', 'longitude']].max().values.tolist()\n",
        "\n",
        "m = folium.Map(location = center, tiles='openstreetmap')\n",
        "\n",
        "# I can add marker one by one on the map\n",
        "for i in range(0,len(gdf)):\n",
        "    folium.Marker([gdf.iloc[i]['latitude'], gdf.iloc[i]['longitude']], popup=gdf.iloc[i]['name']).add_to(m)\n",
        "\n",
        "m.fit_bounds([sw, ne])\n",
        "display(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX4011aO3h4f"
      },
      "source": [
        "# Project\n",
        "\n",
        "The objective of this project is to practice all concepts taught in the main lecture. Therefore, you will have to collect data around a thematic, identify a problematic, clean and format the data, provide some exploratory analysis and visualization, build models and evaluate them. You will have also to design dashboard and to storytell the whole pipeline.\n",
        "\n",
        "\n",
        "The whole project will have two outputs describing the methodology and the results:\n",
        "* A technical report targeting your datascientist colleagues\n",
        "* An oral presentation targeting your CEO, chief, client. In this case, we assume that the audience is not specialized in data science. (But you also need to present the methodology to convince them to trust the results).\n",
        "\n",
        "\n",
        "**Requests:**\n",
        "* Team work of two people (same group throughout the semester)\n",
        "* All your work should be stored on a git repo: [tutorial](https://github.com/baskiotisn/2IN013robot2023/blob/d979333fb80c9b6acd9515aaec040943d10d365c/docs/tutoriel_git.pdf)\n",
        "\n",
        "**Remarks:**\n",
        "There are also other libraries or issues you'll encounter when collecting data. Some of these are listed below, but don't be shy and interact with a search engine to solve your own issue!\n",
        "* [Regular expressions](https://docs.python.org/3/howto/regex.html) might be useful!\n",
        "* [API with authentication](https://www.geeksforgeeks.org/authentication-using-python-requests/)\n",
        "*   [Selenium](https://selenium-python.readthedocs.io/) when pages are generated wia javascript scripts\n",
        "* [Playwright](https://playwright.dev/) --> looks easier and more adapted than Selenium\n",
        "* [Scrapy](https://scrapy.org/) for web crawling / or when you don't know the URL.[Tuto here](https://doc.scrapy.org/en/latest/intro/tutorial.html)\n",
        "* [Summary of some difficulties in scrapping data from the web](https://www.zenrows.com/blog/web-scraping-challenges#page-structure-changes)\n",
        "\n",
        "## Your daily task\n",
        "Collecting data from the web and open data portals. Open data might include csv files, you can use them, but be aware that scraping should be your main acitivity in the dataset gathering.\n",
        "You can begin to format the data, merge them to build dataframe that would be analyzed next week."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
