{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kadem\\.pyenv\\pyenv-win\\versions\\3.9.9\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kadem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kadem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "from datasets import load_from_disk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "import time\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charger le dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prétraitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation et préparation des tenseurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Vectorisation avec TF-IDF et entraînement du SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_examples at 0x000001B80EF24040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|██████████| 20360/20360 [00:00<00:00, 54290.43 examples/s]\n",
      "Map: 100%|██████████| 2733/2733 [00:00<00:00, 48391.32 examples/s]\n",
      "Map: 100%|██████████| 6165/6165 [00:00<00:00, 56713.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "wikiqa_data = load_from_disk(\"wikiqa\")\n",
    "test_data_set = wikiqa_data[\"test\"]\n",
    "train_data_set = wikiqa_data[\"train\"]\n",
    "validation_data_set = wikiqa_data[\"validation\"]\n",
    "\n",
    "\n",
    "def preprocess_examples(examples):\n",
    "    examples['question'] = [utils.preprocess(q) for q in examples['question']]\n",
    "    examples['answer'] = [utils.preprocess(a) for a in examples['answer']]\n",
    "    return examples\n",
    "\n",
    "train_data_set = train_data_set.map(preprocess_examples, batched=True)\n",
    "validation_data_set = validation_data_set.map(preprocess_examples, batched=True)\n",
    "test_data_set = test_data_set.map(preprocess_examples, batched=True)\n",
    "\n",
    "# Convertir en DataFrame\n",
    "train_df = pd.DataFrame({\n",
    "    'question': train_data_set['question'],\n",
    "    'answer': train_data_set['answer'],\n",
    "    'label': train_data_set['label']\n",
    "})\n",
    "\n",
    "validation_df = pd.DataFrame({\n",
    "    'question': validation_data_set['question'],\n",
    "    'answer': validation_data_set['answer'],\n",
    "    'label': validation_data_set['label']\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'question': test_data_set['question'],\n",
    "    'answer': test_data_set['answer'],\n",
    "    'label': test_data_set['label']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble de données d'entraînement :\n",
      "label\n",
      "0    19320\n",
      "1    19320\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Ensemble de données de validation :\n",
      "label\n",
      "0    2593\n",
      "1    2593\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Ensemble de données de test :\n",
      "label\n",
      "0    5872\n",
      "1    5872\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sur-échantillonner la classe minoritaire\n",
    "def balance_classes(df):\n",
    "    df_majority = df[df.label == 0]\n",
    "    df_minority = df[df.label == 1]\n",
    "    \n",
    "    df_minority_upsampled = resample(df_minority, \n",
    "                                     replace=True,     # échantillonner avec remplacement\n",
    "                                     n_samples=len(df_majority),    # pour faire correspondre la classe majoritaire\n",
    "                                     random_state=123) # pour la reproductibilité\n",
    "    \n",
    "    return pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "balanced_train_df = balance_classes(train_df)\n",
    "balanced_validation_df = balance_classes(validation_df)\n",
    "balanced_test_df = balance_classes(test_df)\n",
    "\n",
    "# Afficher les statistiques des ensembles de données après suréchantillonnage\n",
    "print(\"Ensemble de données d'entraînement :\")\n",
    "print(balanced_train_df['label'].value_counts())\n",
    "print(\"\\nEnsemble de données de validation :\")\n",
    "print(balanced_validation_df['label'].value_counts())\n",
    "print(\"\\nEnsemble de données de test :\")\n",
    "print(balanced_test_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble de données d'entraînement :\n",
      "label\n",
      "0    4830\n",
      "1    4830\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Ensemble de données de validation :\n",
      "label\n",
      "0    648\n",
      "1    648\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Ensemble de données de test :\n",
      "label\n",
      "0    1468\n",
      "1    1468\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Réduire la classe majoritaire et suréchantillonner la classe minoritaire\n",
    "def reduce_and_balance_classes(df):\n",
    "    df_majority = df[df.label == 0]\n",
    "    df_minority = df[df.label == 1]\n",
    "    \n",
    "    # Réduire la classe majoritaire de 75 %\n",
    "    df_majority_reduced = resample(df_majority, \n",
    "                                   replace=False,    # échantillonner sans remplacement\n",
    "                                   n_samples=int(len(df_majority) * 0.25),  # 25 % de la classe majoritaire\n",
    "                                   random_state=123) # pour la reproductibilité\n",
    "    \n",
    "    # Suréchantillonner la classe minoritaire pour correspondre à la taille de la classe majoritaire réduite\n",
    "    df_minority_upsampled = resample(df_minority, \n",
    "                                     replace=True,     # échantillonner avec remplacement\n",
    "                                     n_samples=len(df_majority_reduced),    # pour faire correspondre la classe majoritaire réduite\n",
    "                                     random_state=123) # pour la reproductibilité\n",
    "    \n",
    "    return pd.concat([df_majority_reduced, df_minority_upsampled])\n",
    "\n",
    "balanced_train_df = reduce_and_balance_classes(train_df)\n",
    "balanced_validation_df = reduce_and_balance_classes(validation_df)\n",
    "balanced_test_df = reduce_and_balance_classes(test_df)\n",
    "\n",
    "# Afficher les statistiques des ensembles de données après réduction et suréchantillonnage\n",
    "print(\"Ensemble de données d'entraînement :\")\n",
    "print(balanced_train_df['label'].value_counts())\n",
    "print(\"\\nEnsemble de données de validation :\")\n",
    "print(balanced_validation_df['label'].value_counts())\n",
    "print(\"\\nEnsemble de données de test :\")\n",
    "print(balanced_test_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Définir le pipeline de transformation des colonnes\n",
    "preprocess_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('question', TfidfVectorizer(), 'question'),\n",
    "        ('answer', TfidfVectorizer(), 'answer'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocess_pipeline),\n",
    "    ('classifier', SVC(kernel='linear', class_weight='balanced', probability=True))\n",
    "])\n",
    "\n",
    "# Entraîner le modèle avec les données équilibrées\n",
    "start_time = time.time()\n",
    "pipeline.fit(balanced_train_df[['question', 'answer']], balanced_train_df['label'])\n",
    "end_time = time.time()\n",
    "actual_training_time = end_time - start_time\n",
    "print(f\"Temps d'entraînement réel: {actual_training_time:.2f} secondes\")\n",
    "\n",
    "# Fonction pour calculer MAP, MRR, S@1\n",
    "def compute_metrics(df, predictions, probabilities):\n",
    "    grouped = df.groupby('question')\n",
    "    ap_sum = 0\n",
    "    rr_sum = 0\n",
    "    success_at_1 = 0\n",
    "    for name, group in grouped:\n",
    "        group_probs = probabilities[group.index, 1]  # Probabilités de la classe 1\n",
    "        group_labels = group['label'].values\n",
    "        sorted_indices = np.argsort(-group_probs)  # Trier les indices par probabilité décroissante\n",
    "        sorted_labels = group_labels[sorted_indices]\n",
    "        \n",
    "        ap = 0\n",
    "        correct_count = 0\n",
    "        for rank, label in enumerate(sorted_labels, 1):\n",
    "            if label == 1:\n",
    "                correct_count += 1\n",
    "                ap += correct_count / rank\n",
    "                if correct_count == 1:\n",
    "                    rr_sum += 1 / rank\n",
    "        ap /= max(correct_count, 1)\n",
    "        ap_sum += ap\n",
    "        \n",
    "        if sorted_labels[0] == 1:\n",
    "            success_at_1 += 1\n",
    "    \n",
    "    n = len(grouped)\n",
    "    map_score = ap_sum / n\n",
    "    mrr_score = rr_sum / n\n",
    "    success_at_1_score = success_at_1 / n\n",
    "    \n",
    "    return map_score, mrr_score, success_at_1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Évaluer le modèle sur les données de validation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m validation_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(balanced_validation_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m      3\u001b[0m validation_probabilities \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict_proba(balanced_validation_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m      4\u001b[0m map_score, mrr_score, success_at_1_score \u001b[38;5;241m=\u001b[39m compute_metrics(balanced_validation_df, validation_predictions, validation_probabilities)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Évaluer le modèle sur les données de validation\n",
    "validation_predictions = pipeline.predict(balanced_validation_df[['question', 'answer']])\n",
    "validation_probabilities = pipeline.predict_proba(balanced_validation_df[['question', 'answer']])\n",
    "map_score, mrr_score, success_at_1_score = compute_metrics(balanced_validation_df, validation_predictions, validation_probabilities)\n",
    "print(f\"\\nValidation Results:\\nMAP: {map_score}\\nMRR: {mrr_score}\\nS@1: {success_at_1_score}\\n\")\n",
    "\n",
    "# Calculer les métriques classiques\n",
    "validation_accuracy = accuracy_score(balanced_validation_df['label'], validation_predictions)\n",
    "validation_precision = precision_score(balanced_validation_df['label'], validation_predictions)\n",
    "validation_recall = recall_score(balanced_validation_df['label'], validation_predictions)\n",
    "validation_f1 = f1_score(balanced_validation_df['label'], validation_predictions)\n",
    "\n",
    "print(f\"Accuracy: {validation_accuracy}\")\n",
    "print(f\"Precision: {validation_precision}\")\n",
    "print(f\"Recall: {validation_recall}\")\n",
    "print(f\"F1 Score: {validation_f1}\")\n",
    "\n",
    "# Évaluer le modèle sur les données de test\n",
    "test_predictions = pipeline.predict(balanced_test_df[['question', 'answer']])\n",
    "test_probabilities = pipeline.predict_proba(balanced_test_df[['question', 'answer']])\n",
    "map_score, mrr_score, success_at_1_score = compute_metrics(balanced_test_df, test_predictions, test_probabilities)\n",
    "print(f\"\\nTest Results:\\nMAP: {map_score}\\nMRR: {mrr_score}\\nS@1: {success_at_1_score}\\n\")\n",
    "\n",
    "# Calculer les métriques classiques\n",
    "test_accuracy = accuracy_score(balanced_test_df['label'], test_predictions)\n",
    "test_precision = precision_score(balanced_test_df['label'], test_predictions)\n",
    "test_recall = recall_score(balanced_test_df['label'], test_predictions)\n",
    "test_f1 = f1_score(balanced_test_df['label'], test_predictions)\n",
    "\n",
    "print(f\"Accuracy: {test_accuracy}\")\n",
    "print(f\"Precision: {test_precision}\")\n",
    "print(f\"Recall: {test_recall}\")\n",
    "print(f\"F1 Score: {test_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
